# -*- coding: utf-8 -*-
"""Yes-Theory-YouTube-API-Scrapping

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fgva44g-6VDfFSaG6YPOb68kXKKyC8CC
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from googleapiclient.discovery import build
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

# %matplotlib inline
import seaborn as sns
import warnings;
warnings.filterwarnings('ignore')
import datetime
import random
from google.colab import files
import io

api_key = 'AIzaSyCTE-thWkCMJph5hFMgBSTBqdmfkNyWKIw'
ChannelIds = ['UCvK4bOhULCpmLabd2pDMtnA', 'UC-nhLOLf7GVWxN2ntqJscmA','UCSZ3GYzaZlMNNxFsIslehoQ','UCTd7KzdwnFE3lm6LCfYDmUQ']
youtube = build('youtube','v3', developerKey=api_key)

def get_channel_info_func(youtube, channel_id):
    channels_list = []
    request = youtube.channels().list(part='snippet,contentDetails,statistics',id=','.join(channel_id))
    response = request.execute()
    for index in response['items']:
        data = dict(channel_id = index['id'],
               playlist_id = index['contentDetails']['relatedPlaylists']['uploads'],
               channel_name = index['snippet']['title'],
               subscribers = index['statistics']['subscriberCount'],
               total_views = index['statistics']['viewCount'],
               total_videos = index['statistics']['videoCount'])
        channels_list.append(data)
    return channels_list

channel_info_ = get_channel_info_func(youtube, ChannelIds)
channel_info_

df_channels = pd.DataFrame(channel_info_)
df_channels.to_csv('yestheory_channels_info.csv')
files.download('yestheory_channels_info.csv')

def get_playlist_info_func(youtube, playlist_ids):

    request = youtube.playlistItems().list(
        part='contentDetails',
        playlistId=playlist_ids,
        maxResults=50)
    response = request.execute()

    vids_ids = []

    for index in range(len(response['items'])):
        vids_ids.append(response['items'][index]['contentDetails']['videoId'])

    next_page_token = response.get('nextPageToken')
    more_pages = True

    while more_pages:
        if next_page_token is None:
            more_pages = False
        else:
            request = youtube.playlistItems().list(
                part='contentDetails',
                playlistId=playlist_ids,
                maxResults=50,
                pageToken=next_page_token)
            response = request.execute()

            for index in range(len(response['items'])):
                vids_ids.append(response['items'][index]['contentDetails']['videoId'])

        next_page_token = response.get('nextPageToken')

    return vids_ids

PlaylistIds = df_channels['playlist_id'].to_list()


VidsIds = []

for pl_id in range(len(PlaylistIds)):
            VidsIds.append(get_playlist_info_func(youtube, PlaylistIds[pl_id]))
len(VidsIds)

def get_vids_info_func(youtube, vids_ids):

    vids_stat = []

    for i in range(0, len(vids_ids), 50):
      try:
        request = youtube.videos().list(part='snippet,contentDetails,statistics', id=','.join(vids_ids[i:i+50]))
        response = request.execute()

        for video in response['items']:
          vids_statistics = dict(Channel_id = video['snippet']['channelId'],
                                 Channel_name = video['snippet']['channelTitle'],
                                 Video_id = video['id'],
                                 Title = video['snippet']['title'],
                                 Description = video['snippet']['description'],
                                 Duration = video['contentDetails']['duration'],
                                 Definition = video['contentDetails']['definition'],
                                 Caption = video['contentDetails']['caption'],
                                 Published_at = video['snippet']['publishedAt'],
                                 Views = video['statistics']['viewCount'],
                                 Likes = video['statistics']['likeCount'],
                                 Comments = video['statistics']['commentCount'])
          vids_stat.append(vids_statistics)

      except:

        print('failed to fetching data')

    return vids_stat

VidsIds[0][:10]

def get_vids_info_func(youtube, vids_ids):

    vids_stat = []

    for i in range(len(vids_ids)):
        for ii in range(0, len(vids_ids[i]), 50):
            try:
                request = youtube.videos().list(
                    part='snippet,contentDetails,statistics',
                    id=','.join(vids_ids[i][ii:ii+50]))
                response = request.execute()

                for video in response['items']:
                    vids_statistics = dict(Channel_id = video['snippet']['channelId'],
                                           Channel_name = video['snippet']['channelTitle'],
                                           Video_id = video['id'],
                                           Title = video['snippet']['title'],
                                           Description = video['snippet']['description'],
                                           Duration = video['contentDetails']['duration'],
                                           Definition = video['contentDetails']['definition'],
                                           Caption = video['contentDetails']['caption'],
                                           Published_at = video['snippet']['publishedAt'],
                                           Views = video['statistics']['viewCount'],
                                           Likes = video['statistics']['likeCount'],
                                           Comments = video['statistics']['commentCount'])
                    vids_stat.append(vids_statistics)
            except:
                print('failed to fetching data')

    return vids_stat

vids_info_ = get_vids_info_func(youtube, VidsIds)
vids_info_

vids_df  = pd.DataFrame(vids_info_)
vids_df.shape

vids_df.to_csv('yestheory_videos_info.csv')
files.download('yestheory_videos_info.csv')

def get_comments_info_func(youtube, vids_ids):

    all_comments = []

    for i in range(len(vids_ids)):
        for j in range(len(vids_ids[i])):
            try:
                request = youtube.commentThreads().list(
                    part='snippet,replies',
                    videoId=vids_ids[i][j])
                response = request.execute()

                for comment in response['items']:
                    comments_info = {'Video_id' : vids_ids[i][j],
                                     'Comments' : comment['snippet']['topLevelComment']['snippet']['textOriginal']}
                    all_comments.append(comments_info)

            except:
                error = f'failed to fetch data for video id : {vids_ids[i][j]}'
                print(error)

    return all_comments

AllComments = get_comments_info_func(youtube, VidsIds)
AllComments

comments_df = pd.DataFrame(AllComments)
comments_df.to_csv('yestheory_raw_comments.csv')
files.download('yestheory_raw_comments.csv')